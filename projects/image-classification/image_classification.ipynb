{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f67b023be48>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x/255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer().fit(range(0,10))\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return encoder.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = [None, *image_shape], name = 'x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = [None, n_classes], name = 'y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name = 'keep_prob') \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    batch_size, x_height, x_width, x_depth=x_tensor.get_shape().as_list()\n",
    "    \n",
    "    weight = tf.Variable(tf.truncated_normal([*conv_ksize, x_depth, conv_num_outputs], stddev=0.01))\n",
    "    \n",
    "    bias = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    \n",
    "    _strides = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides=_strides, padding='SAME')\n",
    "    \n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    _ksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    _strides = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize=_ksize, strides=_strides, padding='SAME')\n",
    "    \n",
    "    return conv_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=tf.nn.relu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    nn = conv2d_maxpool(x, 32, (3,3),(1,1),(3,3),(1,1))\n",
    "    nn = conv2d_maxpool(nn, 64, (3,3),(1,1),(3,3),(1,1))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    nn = flatten(nn)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    nn = fully_conn(nn, 1024)\n",
    "    nn = tf.nn.dropout(nn, keep_prob)\n",
    "    \n",
    "    nn = fully_conn(nn, 256)\n",
    "    nn = tf.nn.dropout(nn, keep_prob)\n",
    "    \n",
    "    nn = fully_conn(nn, 100)\n",
    "    nn = tf.nn.dropout(nn, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    nn = output(nn, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return nn\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    _feed_dict = {x: feature_batch,\n",
    "                  y: label_batch,\n",
    "                  keep_prob: keep_probability}\n",
    "    \n",
    "    session.run(optimizer, feed_dict=_feed_dict)\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    valid_loss = session.run(cost, feed_dict={\n",
    "        x: valid_features,\n",
    "        y: valid_labels,\n",
    "        keep_prob: 1.0})\n",
    "    \n",
    "    train_loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.0})    \n",
    "    \n",
    "    train_acc = session.run(accuracy, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.0})\n",
    "\n",
    "    valid_acc = session.run(accuracy, feed_dict={\n",
    "        x: valid_features,\n",
    "        y: valid_labels,\n",
    "        keep_prob: 1.0})    \n",
    "\n",
    "    print('Validation Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "              valid_loss,valid_acc))\n",
    "    print('Train Loss: {:>10.4f} Train Accuracy: {:.6f}'.format(\n",
    "              train_loss,train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Validation Loss:     1.9683 Validation Accuracy: 0.289600\n",
      "Train Loss:     2.1678 Train Accuracy: 0.225000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Validation Loss:     1.8008 Validation Accuracy: 0.346000\n",
      "Train Loss:     1.9190 Train Accuracy: 0.400000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Validation Loss:     1.6120 Validation Accuracy: 0.419000\n",
      "Train Loss:     1.7964 Train Accuracy: 0.450000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Validation Loss:     1.5178 Validation Accuracy: 0.445800\n",
      "Train Loss:     1.6630 Train Accuracy: 0.425000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Validation Loss:     1.5184 Validation Accuracy: 0.455600\n",
      "Train Loss:     1.5370 Train Accuracy: 0.550000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Validation Loss:     1.4598 Validation Accuracy: 0.475000\n",
      "Train Loss:     1.3898 Train Accuracy: 0.550000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Validation Loss:     1.4091 Validation Accuracy: 0.488800\n",
      "Train Loss:     1.2226 Train Accuracy: 0.600000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Validation Loss:     1.3820 Validation Accuracy: 0.504000\n",
      "Train Loss:     1.1237 Train Accuracy: 0.550000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Validation Loss:     1.4111 Validation Accuracy: 0.499200\n",
      "Train Loss:     0.8619 Train Accuracy: 0.725000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Validation Loss:     1.3685 Validation Accuracy: 0.510800\n",
      "Train Loss:     0.7590 Train Accuracy: 0.775000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Validation Loss:     1.3532 Validation Accuracy: 0.529800\n",
      "Train Loss:     0.6815 Train Accuracy: 0.750000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Validation Loss:     1.3609 Validation Accuracy: 0.534400\n",
      "Train Loss:     0.5835 Train Accuracy: 0.825000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Validation Loss:     1.4794 Validation Accuracy: 0.504400\n",
      "Train Loss:     0.5899 Train Accuracy: 0.850000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Validation Loss:     1.3446 Validation Accuracy: 0.549200\n",
      "Train Loss:     0.4815 Train Accuracy: 0.850000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Validation Loss:     1.3736 Validation Accuracy: 0.543400\n",
      "Train Loss:     0.4537 Train Accuracy: 0.875000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Validation Loss:     1.4320 Validation Accuracy: 0.538600\n",
      "Train Loss:     0.3309 Train Accuracy: 0.875000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Validation Loss:     1.4941 Validation Accuracy: 0.522200\n",
      "Train Loss:     0.3325 Train Accuracy: 0.950000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Validation Loss:     1.5629 Validation Accuracy: 0.532600\n",
      "Train Loss:     0.2605 Train Accuracy: 0.925000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Validation Loss:     1.5734 Validation Accuracy: 0.521400\n",
      "Train Loss:     0.2238 Train Accuracy: 0.950000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Validation Loss:     1.5849 Validation Accuracy: 0.534600\n",
      "Train Loss:     0.1762 Train Accuracy: 0.950000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Validation Loss:     1.5068 Validation Accuracy: 0.537200\n",
      "Train Loss:     0.1647 Train Accuracy: 0.975000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Validation Loss:     1.6497 Validation Accuracy: 0.533400\n",
      "Train Loss:     0.1146 Train Accuracy: 0.975000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Validation Loss:     1.7126 Validation Accuracy: 0.538000\n",
      "Train Loss:     0.0606 Train Accuracy: 1.000000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Validation Loss:     1.7808 Validation Accuracy: 0.534800\n",
      "Train Loss:     0.0716 Train Accuracy: 1.000000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Validation Loss:     1.7546 Validation Accuracy: 0.546200\n",
      "Train Loss:     0.0651 Train Accuracy: 0.975000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Validation Loss:     1.8329 Validation Accuracy: 0.543200\n",
      "Train Loss:     0.0317 Train Accuracy: 1.000000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Validation Loss:     1.8540 Validation Accuracy: 0.551000\n",
      "Train Loss:     0.0300 Train Accuracy: 1.000000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Validation Loss:     1.9076 Validation Accuracy: 0.539200\n",
      "Train Loss:     0.0299 Train Accuracy: 1.000000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Validation Loss:     2.1168 Validation Accuracy: 0.516000\n",
      "Train Loss:     0.0220 Train Accuracy: 1.000000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Validation Loss:     1.9211 Validation Accuracy: 0.541200\n",
      "Train Loss:     0.0315 Train Accuracy: 1.000000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Validation Loss:     2.0518 Validation Accuracy: 0.535800\n",
      "Train Loss:     0.0134 Train Accuracy: 1.000000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Validation Loss:     2.0866 Validation Accuracy: 0.531600\n",
      "Train Loss:     0.0104 Train Accuracy: 1.000000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Validation Loss:     2.1362 Validation Accuracy: 0.531000\n",
      "Train Loss:     0.0090 Train Accuracy: 1.000000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Validation Loss:     2.1097 Validation Accuracy: 0.541200\n",
      "Train Loss:     0.0157 Train Accuracy: 1.000000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Validation Loss:     2.1688 Validation Accuracy: 0.541400\n",
      "Train Loss:     0.0106 Train Accuracy: 1.000000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Validation Loss:     2.2250 Validation Accuracy: 0.542400\n",
      "Train Loss:     0.0064 Train Accuracy: 1.000000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Validation Loss:     2.2042 Validation Accuracy: 0.550200\n",
      "Train Loss:     0.0059 Train Accuracy: 1.000000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Validation Loss:     2.4011 Validation Accuracy: 0.531400\n",
      "Train Loss:     0.0102 Train Accuracy: 1.000000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Validation Loss:     2.2967 Validation Accuracy: 0.523200\n",
      "Train Loss:     0.0050 Train Accuracy: 1.000000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Validation Loss:     2.3766 Validation Accuracy: 0.528400\n",
      "Train Loss:     0.0090 Train Accuracy: 1.000000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Validation Loss:     2.2382 Validation Accuracy: 0.528800\n",
      "Train Loss:     0.0196 Train Accuracy: 1.000000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Validation Loss:     2.2185 Validation Accuracy: 0.527000\n",
      "Train Loss:     0.0122 Train Accuracy: 1.000000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Validation Loss:     2.3288 Validation Accuracy: 0.535600\n",
      "Train Loss:     0.0019 Train Accuracy: 1.000000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Validation Loss:     2.3891 Validation Accuracy: 0.545800\n",
      "Train Loss:     0.0009 Train Accuracy: 1.000000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Validation Loss:     2.4758 Validation Accuracy: 0.544800\n",
      "Train Loss:     0.0063 Train Accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Validation Loss:     2.5177 Validation Accuracy: 0.524400\n",
      "Train Loss:     0.0022 Train Accuracy: 1.000000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Validation Loss:     2.4905 Validation Accuracy: 0.531000\n",
      "Train Loss:     0.0013 Train Accuracy: 1.000000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Validation Loss:     2.5483 Validation Accuracy: 0.533200\n",
      "Train Loss:     0.0018 Train Accuracy: 1.000000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Validation Loss:     2.5392 Validation Accuracy: 0.539600\n",
      "Train Loss:     0.0019 Train Accuracy: 1.000000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Validation Loss:     2.6121 Validation Accuracy: 0.530000\n",
      "Train Loss:     0.0010 Train Accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Validation Loss:     2.5032 Validation Accuracy: 0.539800\n",
      "Train Loss:     0.0013 Train Accuracy: 1.000000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Validation Loss:     2.5150 Validation Accuracy: 0.544800\n",
      "Train Loss:     0.0005 Train Accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Validation Loss:     2.5579 Validation Accuracy: 0.546000\n",
      "Train Loss:     0.0003 Train Accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Validation Loss:     2.6949 Validation Accuracy: 0.547400\n",
      "Train Loss:     0.0004 Train Accuracy: 1.000000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Validation Loss:     2.6959 Validation Accuracy: 0.551200\n",
      "Train Loss:     0.0003 Train Accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Validation Loss:     2.6661 Validation Accuracy: 0.549400\n",
      "Train Loss:     0.0004 Train Accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Validation Loss:     2.6309 Validation Accuracy: 0.549000\n",
      "Train Loss:     0.0001 Train Accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Validation Loss:     2.6942 Validation Accuracy: 0.553600\n",
      "Train Loss:     0.0001 Train Accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Validation Loss:     2.6976 Validation Accuracy: 0.537600\n",
      "Train Loss:     0.0002 Train Accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Validation Loss:     2.8396 Validation Accuracy: 0.543600\n",
      "Train Loss:     0.0003 Train Accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Validation Loss:     2.5536 Validation Accuracy: 0.539400\n",
      "Train Loss:     0.0002 Train Accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, CIFAR-10 Batch 1:  Validation Loss:     2.7454 Validation Accuracy: 0.535000\n",
      "Train Loss:     0.0002 Train Accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Validation Loss:     2.8117 Validation Accuracy: 0.539600\n",
      "Train Loss:     0.0001 Train Accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Validation Loss:     2.7124 Validation Accuracy: 0.547400\n",
      "Train Loss:     0.0001 Train Accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Validation Loss:     2.8110 Validation Accuracy: 0.551400\n",
      "Train Loss:     0.0006 Train Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Validation Loss:     2.8670 Validation Accuracy: 0.546400\n",
      "Train Loss:     0.0007 Train Accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Validation Loss:     2.8959 Validation Accuracy: 0.544400\n",
      "Train Loss:     0.0006 Train Accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Validation Loss:     2.6868 Validation Accuracy: 0.548400\n",
      "Train Loss:     0.0001 Train Accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Validation Loss:     2.9636 Validation Accuracy: 0.550600\n",
      "Train Loss:     0.0000 Train Accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Validation Loss:     2.6983 Validation Accuracy: 0.541800\n",
      "Train Loss:     0.0002 Train Accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Validation Loss:     2.9159 Validation Accuracy: 0.549000\n",
      "Train Loss:     0.0001 Train Accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Validation Loss:     2.9404 Validation Accuracy: 0.536000\n",
      "Train Loss:     0.0009 Train Accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Validation Loss:     2.8085 Validation Accuracy: 0.538000\n",
      "Train Loss:     0.0004 Train Accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Validation Loss:     2.7655 Validation Accuracy: 0.537600\n",
      "Train Loss:     0.0016 Train Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Validation Loss:     2.7868 Validation Accuracy: 0.549600\n",
      "Train Loss:     0.0005 Train Accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Validation Loss:     2.7644 Validation Accuracy: 0.539600\n",
      "Train Loss:     0.0001 Train Accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Validation Loss:     2.9835 Validation Accuracy: 0.546800\n",
      "Train Loss:     0.0001 Train Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Validation Loss:     2.7933 Validation Accuracy: 0.552400\n",
      "Train Loss:     0.0018 Train Accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Validation Loss:     2.9090 Validation Accuracy: 0.545400\n",
      "Train Loss:     0.0001 Train Accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Validation Loss:     2.9013 Validation Accuracy: 0.544000\n",
      "Train Loss:     0.0005 Train Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Validation Loss:     2.8137 Validation Accuracy: 0.545200\n",
      "Train Loss:     0.0051 Train Accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Validation Loss:     3.0358 Validation Accuracy: 0.545000\n",
      "Train Loss:     0.0001 Train Accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Validation Loss:     2.8991 Validation Accuracy: 0.547000\n",
      "Train Loss:     0.0002 Train Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Validation Loss:     3.0512 Validation Accuracy: 0.548200\n",
      "Train Loss:     0.0000 Train Accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Validation Loss:     2.8323 Validation Accuracy: 0.540800\n",
      "Train Loss:     0.0000 Train Accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Validation Loss:     2.8244 Validation Accuracy: 0.549600\n",
      "Train Loss:     0.0000 Train Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Validation Loss:     2.7688 Validation Accuracy: 0.548000\n",
      "Train Loss:     0.0000 Train Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Validation Loss:     2.8920 Validation Accuracy: 0.547400\n",
      "Train Loss:     0.0000 Train Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Validation Loss:     2.8436 Validation Accuracy: 0.538000\n",
      "Train Loss:     0.0000 Train Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Validation Loss:     2.9111 Validation Accuracy: 0.545600\n",
      "Train Loss:     0.0001 Train Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Validation Loss:     3.0267 Validation Accuracy: 0.540400\n",
      "Train Loss:     0.0001 Train Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Validation Loss:     2.9665 Validation Accuracy: 0.539400\n",
      "Train Loss:     0.0002 Train Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Validation Loss:     3.1042 Validation Accuracy: 0.555600\n",
      "Train Loss:     0.0000 Train Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Validation Loss:     3.0160 Validation Accuracy: 0.544200\n",
      "Train Loss:     0.0013 Train Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Validation Loss:     2.9353 Validation Accuracy: 0.549000\n",
      "Train Loss:     0.0000 Train Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Validation Loss:     2.8649 Validation Accuracy: 0.547800\n",
      "Train Loss:     0.0003 Train Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Validation Loss:     2.9281 Validation Accuracy: 0.544400\n",
      "Train Loss:     0.0000 Train Accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Validation Loss:     2.9452 Validation Accuracy: 0.546800\n",
      "Train Loss:     0.0000 Train Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Validation Loss:     2.9664 Validation Accuracy: 0.540200\n",
      "Train Loss:     0.0000 Train Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Validation Loss:     3.0189 Validation Accuracy: 0.547000\n",
      "Train Loss:     0.0000 Train Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Validation Loss:     2.0525 Validation Accuracy: 0.252200\n",
      "Train Loss:     2.1623 Train Accuracy: 0.200000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Validation Loss:     1.9320 Validation Accuracy: 0.296400\n",
      "Train Loss:     2.0161 Train Accuracy: 0.350000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Validation Loss:     1.7822 Validation Accuracy: 0.344800\n",
      "Train Loss:     1.6095 Train Accuracy: 0.375000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Validation Loss:     1.6930 Validation Accuracy: 0.385600\n",
      "Train Loss:     1.7164 Train Accuracy: 0.300000\n",
      "Epoch  1, CIFAR-10 Batch 5:  Validation Loss:     1.6606 Validation Accuracy: 0.393600\n",
      "Train Loss:     1.7641 Train Accuracy: 0.325000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Validation Loss:     1.6173 Validation Accuracy: 0.415800\n",
      "Train Loss:     1.7646 Train Accuracy: 0.400000\n",
      "Epoch  2, CIFAR-10 Batch 2:  Validation Loss:     1.5830 Validation Accuracy: 0.430000\n",
      "Train Loss:     1.6733 Train Accuracy: 0.475000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Validation Loss:     1.5753 Validation Accuracy: 0.424400\n",
      "Train Loss:     1.4191 Train Accuracy: 0.525000\n",
      "Epoch  2, CIFAR-10 Batch 4:  Validation Loss:     1.5505 Validation Accuracy: 0.437200\n",
      "Train Loss:     1.4527 Train Accuracy: 0.525000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Validation Loss:     1.5103 Validation Accuracy: 0.454800\n",
      "Train Loss:     1.5417 Train Accuracy: 0.475000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Validation Loss:     1.5029 Validation Accuracy: 0.464200\n",
      "Train Loss:     1.6054 Train Accuracy: 0.550000\n",
      "Epoch  3, CIFAR-10 Batch 2:  Validation Loss:     1.4788 Validation Accuracy: 0.476000\n",
      "Train Loss:     1.4749 Train Accuracy: 0.525000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Validation Loss:     1.4698 Validation Accuracy: 0.472800\n",
      "Train Loss:     1.2230 Train Accuracy: 0.525000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Validation Loss:     1.4447 Validation Accuracy: 0.478000\n",
      "Train Loss:     1.2662 Train Accuracy: 0.625000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Validation Loss:     1.4297 Validation Accuracy: 0.487600\n",
      "Train Loss:     1.3880 Train Accuracy: 0.600000\n",
      "Epoch  4, CIFAR-10 Batch 1:  Validation Loss:     1.4270 Validation Accuracy: 0.492600\n",
      "Train Loss:     1.4529 Train Accuracy: 0.575000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Validation Loss:     1.4200 Validation Accuracy: 0.498800\n",
      "Train Loss:     1.3617 Train Accuracy: 0.500000\n",
      "Epoch  4, CIFAR-10 Batch 3:  Validation Loss:     1.4132 Validation Accuracy: 0.498800\n",
      "Train Loss:     1.1020 Train Accuracy: 0.600000\n",
      "Epoch  4, CIFAR-10 Batch 4:  Validation Loss:     1.4102 Validation Accuracy: 0.494800\n",
      "Train Loss:     1.1819 Train Accuracy: 0.650000\n",
      "Epoch  4, CIFAR-10 Batch 5:  Validation Loss:     1.3938 Validation Accuracy: 0.497200\n",
      "Train Loss:     1.2367 Train Accuracy: 0.575000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Validation Loss:     1.3735 Validation Accuracy: 0.516200\n",
      "Train Loss:     1.3274 Train Accuracy: 0.600000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Validation Loss:     1.3717 Validation Accuracy: 0.512600\n",
      "Train Loss:     1.2111 Train Accuracy: 0.525000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Validation Loss:     1.3778 Validation Accuracy: 0.507200\n",
      "Train Loss:     1.0798 Train Accuracy: 0.625000\n",
      "Epoch  5, CIFAR-10 Batch 4:  Validation Loss:     1.3913 Validation Accuracy: 0.506600\n",
      "Train Loss:     1.0725 Train Accuracy: 0.675000\n",
      "Epoch  5, CIFAR-10 Batch 5:  Validation Loss:     1.3480 Validation Accuracy: 0.516600\n",
      "Train Loss:     1.1211 Train Accuracy: 0.550000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Validation Loss:     1.3545 Validation Accuracy: 0.523400\n",
      "Train Loss:     1.2181 Train Accuracy: 0.625000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Validation Loss:     1.3362 Validation Accuracy: 0.521200\n",
      "Train Loss:     1.0874 Train Accuracy: 0.675000\n",
      "Epoch  6, CIFAR-10 Batch 3:  Validation Loss:     1.3532 Validation Accuracy: 0.513600\n",
      "Train Loss:     0.9408 Train Accuracy: 0.675000\n",
      "Epoch  6, CIFAR-10 Batch 4:  Validation Loss:     1.3422 Validation Accuracy: 0.525400\n",
      "Train Loss:     0.9579 Train Accuracy: 0.700000\n",
      "Epoch  6, CIFAR-10 Batch 5:  Validation Loss:     1.3274 Validation Accuracy: 0.524200\n",
      "Train Loss:     1.0592 Train Accuracy: 0.600000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Validation Loss:     1.3545 Validation Accuracy: 0.524000\n",
      "Train Loss:     1.1074 Train Accuracy: 0.625000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Validation Loss:     1.3152 Validation Accuracy: 0.534400\n",
      "Train Loss:     1.0307 Train Accuracy: 0.600000\n",
      "Epoch  7, CIFAR-10 Batch 3:  Validation Loss:     1.3329 Validation Accuracy: 0.524800\n",
      "Train Loss:     0.8686 Train Accuracy: 0.700000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Validation Loss:     1.3207 Validation Accuracy: 0.533800\n",
      "Train Loss:     0.8837 Train Accuracy: 0.700000\n",
      "Epoch  7, CIFAR-10 Batch 5:  Validation Loss:     1.3005 Validation Accuracy: 0.538400\n",
      "Train Loss:     0.9503 Train Accuracy: 0.675000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Validation Loss:     1.3035 Validation Accuracy: 0.537600\n",
      "Train Loss:     0.9740 Train Accuracy: 0.675000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Validation Loss:     1.3140 Validation Accuracy: 0.531200\n",
      "Train Loss:     0.9377 Train Accuracy: 0.650000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Validation Loss:     1.3097 Validation Accuracy: 0.536400\n",
      "Train Loss:     0.8668 Train Accuracy: 0.675000\n",
      "Epoch  8, CIFAR-10 Batch 4:  Validation Loss:     1.2951 Validation Accuracy: 0.541600\n",
      "Train Loss:     0.8669 Train Accuracy: 0.700000\n",
      "Epoch  8, CIFAR-10 Batch 5:  Validation Loss:     1.3084 Validation Accuracy: 0.533600\n",
      "Train Loss:     0.9189 Train Accuracy: 0.725000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Validation Loss:     1.3087 Validation Accuracy: 0.537600\n",
      "Train Loss:     0.9546 Train Accuracy: 0.700000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Validation Loss:     1.2849 Validation Accuracy: 0.553800\n",
      "Train Loss:     0.8709 Train Accuracy: 0.675000\n",
      "Epoch  9, CIFAR-10 Batch 3:  Validation Loss:     1.2803 Validation Accuracy: 0.550800\n",
      "Train Loss:     0.7570 Train Accuracy: 0.775000\n",
      "Epoch  9, CIFAR-10 Batch 4:  Validation Loss:     1.2831 Validation Accuracy: 0.544600\n",
      "Train Loss:     0.7655 Train Accuracy: 0.775000\n",
      "Epoch  9, CIFAR-10 Batch 5:  Validation Loss:     1.2986 Validation Accuracy: 0.535600\n",
      "Train Loss:     0.8053 Train Accuracy: 0.750000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Validation Loss:     1.2792 Validation Accuracy: 0.552600\n",
      "Train Loss:     0.8453 Train Accuracy: 0.725000\n",
      "Epoch 10, CIFAR-10 Batch 2:  Validation Loss:     1.2850 Validation Accuracy: 0.549000\n",
      "Train Loss:     0.7919 Train Accuracy: 0.725000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Validation Loss:     1.2714 Validation Accuracy: 0.553400\n",
      "Train Loss:     0.6940 Train Accuracy: 0.725000\n",
      "Epoch 10, CIFAR-10 Batch 4:  Validation Loss:     1.2794 Validation Accuracy: 0.546600\n",
      "Train Loss:     0.7803 Train Accuracy: 0.800000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Validation Loss:     1.2667 Validation Accuracy: 0.554800\n",
      "Train Loss:     0.7024 Train Accuracy: 0.800000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Validation Loss:     1.2794 Validation Accuracy: 0.544600\n",
      "Train Loss:     0.7758 Train Accuracy: 0.750000\n",
      "Epoch 11, CIFAR-10 Batch 2:  Validation Loss:     1.2541 Validation Accuracy: 0.558800\n",
      "Train Loss:     0.7345 Train Accuracy: 0.750000\n",
      "Epoch 11, CIFAR-10 Batch 3:  Validation Loss:     1.2549 Validation Accuracy: 0.552400\n",
      "Train Loss:     0.6264 Train Accuracy: 0.825000\n",
      "Epoch 11, CIFAR-10 Batch 4:  Validation Loss:     1.2712 Validation Accuracy: 0.548800\n",
      "Train Loss:     0.7239 Train Accuracy: 0.750000\n",
      "Epoch 11, CIFAR-10 Batch 5:  Validation Loss:     1.3189 Validation Accuracy: 0.529600\n",
      "Train Loss:     0.7498 Train Accuracy: 0.775000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Validation Loss:     1.2507 Validation Accuracy: 0.552600\n",
      "Train Loss:     0.8231 Train Accuracy: 0.700000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Validation Loss:     1.2826 Validation Accuracy: 0.555000\n",
      "Train Loss:     0.6783 Train Accuracy: 0.750000\n",
      "Epoch 12, CIFAR-10 Batch 3:  Validation Loss:     1.2862 Validation Accuracy: 0.552800\n",
      "Train Loss:     0.5909 Train Accuracy: 0.800000\n",
      "Epoch 12, CIFAR-10 Batch 4:  Validation Loss:     1.2786 Validation Accuracy: 0.547200\n",
      "Train Loss:     0.6791 Train Accuracy: 0.800000\n",
      "Epoch 12, CIFAR-10 Batch 5:  Validation Loss:     1.3048 Validation Accuracy: 0.537600\n",
      "Train Loss:     0.6859 Train Accuracy: 0.800000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Validation Loss:     1.2487 Validation Accuracy: 0.553200\n",
      "Train Loss:     0.6987 Train Accuracy: 0.800000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, CIFAR-10 Batch 2:  Validation Loss:     1.2656 Validation Accuracy: 0.558800\n",
      "Train Loss:     0.5900 Train Accuracy: 0.800000\n",
      "Epoch 13, CIFAR-10 Batch 3:  Validation Loss:     1.2765 Validation Accuracy: 0.551800\n",
      "Train Loss:     0.5356 Train Accuracy: 0.900000\n",
      "Epoch 13, CIFAR-10 Batch 4:  Validation Loss:     1.2583 Validation Accuracy: 0.551000\n",
      "Train Loss:     0.6195 Train Accuracy: 0.825000\n",
      "Epoch 13, CIFAR-10 Batch 5:  Validation Loss:     1.3112 Validation Accuracy: 0.545000\n",
      "Train Loss:     0.6620 Train Accuracy: 0.800000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Validation Loss:     1.2482 Validation Accuracy: 0.554400\n",
      "Train Loss:     0.7194 Train Accuracy: 0.725000\n",
      "Epoch 14, CIFAR-10 Batch 2:  Validation Loss:     1.2594 Validation Accuracy: 0.560000\n",
      "Train Loss:     0.5832 Train Accuracy: 0.825000\n",
      "Epoch 14, CIFAR-10 Batch 3:  Validation Loss:     1.2673 Validation Accuracy: 0.557000\n",
      "Train Loss:     0.4997 Train Accuracy: 0.875000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Validation Loss:     1.2568 Validation Accuracy: 0.553600\n",
      "Train Loss:     0.5704 Train Accuracy: 0.900000\n",
      "Epoch 14, CIFAR-10 Batch 5:  Validation Loss:     1.2776 Validation Accuracy: 0.557600\n",
      "Train Loss:     0.5234 Train Accuracy: 0.875000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Validation Loss:     1.2557 Validation Accuracy: 0.560200\n",
      "Train Loss:     0.6199 Train Accuracy: 0.850000\n",
      "Epoch 15, CIFAR-10 Batch 2:  Validation Loss:     1.2734 Validation Accuracy: 0.558200\n",
      "Train Loss:     0.4819 Train Accuracy: 0.850000\n",
      "Epoch 15, CIFAR-10 Batch 3:  Validation Loss:     1.2777 Validation Accuracy: 0.551800\n",
      "Train Loss:     0.4752 Train Accuracy: 0.925000\n",
      "Epoch 15, CIFAR-10 Batch 4:  Validation Loss:     1.2603 Validation Accuracy: 0.551800\n",
      "Train Loss:     0.5405 Train Accuracy: 0.850000\n",
      "Epoch 15, CIFAR-10 Batch 5:  Validation Loss:     1.2743 Validation Accuracy: 0.550200\n",
      "Train Loss:     0.5005 Train Accuracy: 0.925000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Validation Loss:     1.2767 Validation Accuracy: 0.550800\n",
      "Train Loss:     0.5943 Train Accuracy: 0.800000\n",
      "Epoch 16, CIFAR-10 Batch 2:  Validation Loss:     1.2919 Validation Accuracy: 0.551200\n",
      "Train Loss:     0.4985 Train Accuracy: 0.875000\n",
      "Epoch 16, CIFAR-10 Batch 3:  Validation Loss:     1.3021 Validation Accuracy: 0.543600\n",
      "Train Loss:     0.5245 Train Accuracy: 0.850000\n",
      "Epoch 16, CIFAR-10 Batch 4:  Validation Loss:     1.2697 Validation Accuracy: 0.554200\n",
      "Train Loss:     0.4645 Train Accuracy: 0.925000\n",
      "Epoch 16, CIFAR-10 Batch 5:  Validation Loss:     1.2500 Validation Accuracy: 0.568600\n",
      "Train Loss:     0.4623 Train Accuracy: 0.825000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Validation Loss:     1.2743 Validation Accuracy: 0.550200\n",
      "Train Loss:     0.5347 Train Accuracy: 0.850000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Validation Loss:     1.3065 Validation Accuracy: 0.554000\n",
      "Train Loss:     0.4463 Train Accuracy: 0.900000\n",
      "Epoch 17, CIFAR-10 Batch 3:  Validation Loss:     1.2982 Validation Accuracy: 0.557800\n",
      "Train Loss:     0.4293 Train Accuracy: 0.900000\n",
      "Epoch 17, CIFAR-10 Batch 4:  Validation Loss:     1.2732 Validation Accuracy: 0.554000\n",
      "Train Loss:     0.4768 Train Accuracy: 0.875000\n",
      "Epoch 17, CIFAR-10 Batch 5:  Validation Loss:     1.2660 Validation Accuracy: 0.564800\n",
      "Train Loss:     0.3774 Train Accuracy: 0.875000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Validation Loss:     1.2811 Validation Accuracy: 0.556400\n",
      "Train Loss:     0.4500 Train Accuracy: 0.900000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Validation Loss:     1.3158 Validation Accuracy: 0.549600\n",
      "Train Loss:     0.4379 Train Accuracy: 0.875000\n",
      "Epoch 18, CIFAR-10 Batch 3:  Validation Loss:     1.2703 Validation Accuracy: 0.561800\n",
      "Train Loss:     0.4063 Train Accuracy: 0.900000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Validation Loss:     1.2388 Validation Accuracy: 0.564000\n",
      "Train Loss:     0.4519 Train Accuracy: 0.900000\n",
      "Epoch 18, CIFAR-10 Batch 5:  Validation Loss:     1.2412 Validation Accuracy: 0.565000\n",
      "Train Loss:     0.3836 Train Accuracy: 0.950000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Validation Loss:     1.2885 Validation Accuracy: 0.550400\n",
      "Train Loss:     0.4497 Train Accuracy: 0.850000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Validation Loss:     1.3454 Validation Accuracy: 0.545000\n",
      "Train Loss:     0.4375 Train Accuracy: 0.900000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Validation Loss:     1.2716 Validation Accuracy: 0.560000\n",
      "Train Loss:     0.4124 Train Accuracy: 0.900000\n",
      "Epoch 19, CIFAR-10 Batch 4:  Validation Loss:     1.2632 Validation Accuracy: 0.559800\n",
      "Train Loss:     0.4090 Train Accuracy: 0.950000\n",
      "Epoch 19, CIFAR-10 Batch 5:  Validation Loss:     1.2661 Validation Accuracy: 0.566000\n",
      "Train Loss:     0.3567 Train Accuracy: 0.925000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Validation Loss:     1.2767 Validation Accuracy: 0.555000\n",
      "Train Loss:     0.4311 Train Accuracy: 0.875000\n",
      "Epoch 20, CIFAR-10 Batch 2:  Validation Loss:     1.2368 Validation Accuracy: 0.567800\n",
      "Train Loss:     0.4381 Train Accuracy: 0.925000\n",
      "Epoch 20, CIFAR-10 Batch 3:  Validation Loss:     1.2771 Validation Accuracy: 0.555800\n",
      "Train Loss:     0.3747 Train Accuracy: 0.925000\n",
      "Epoch 20, CIFAR-10 Batch 4:  Validation Loss:     1.2737 Validation Accuracy: 0.557800\n",
      "Train Loss:     0.4103 Train Accuracy: 0.925000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Validation Loss:     1.2626 Validation Accuracy: 0.567000\n",
      "Train Loss:     0.2913 Train Accuracy: 0.925000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Validation Loss:     1.2838 Validation Accuracy: 0.555600\n",
      "Train Loss:     0.5214 Train Accuracy: 0.900000\n",
      "Epoch 21, CIFAR-10 Batch 2:  Validation Loss:     1.2524 Validation Accuracy: 0.570000\n",
      "Train Loss:     0.3836 Train Accuracy: 0.925000\n",
      "Epoch 21, CIFAR-10 Batch 3:  Validation Loss:     1.2698 Validation Accuracy: 0.561200\n",
      "Train Loss:     0.3161 Train Accuracy: 0.975000\n",
      "Epoch 21, CIFAR-10 Batch 4:  Validation Loss:     1.2824 Validation Accuracy: 0.563400\n",
      "Train Loss:     0.3703 Train Accuracy: 0.950000\n",
      "Epoch 21, CIFAR-10 Batch 5:  Validation Loss:     1.2748 Validation Accuracy: 0.569800\n",
      "Train Loss:     0.2776 Train Accuracy: 0.950000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Validation Loss:     1.2662 Validation Accuracy: 0.571000\n",
      "Train Loss:     0.4336 Train Accuracy: 0.900000\n",
      "Epoch 22, CIFAR-10 Batch 2:  Validation Loss:     1.2678 Validation Accuracy: 0.566000\n",
      "Train Loss:     0.3554 Train Accuracy: 0.925000\n",
      "Epoch 22, CIFAR-10 Batch 3:  Validation Loss:     1.3004 Validation Accuracy: 0.558200\n",
      "Train Loss:     0.2854 Train Accuracy: 0.950000\n",
      "Epoch 22, CIFAR-10 Batch 4:  Validation Loss:     1.2927 Validation Accuracy: 0.561200\n",
      "Train Loss:     0.3626 Train Accuracy: 0.975000\n",
      "Epoch 22, CIFAR-10 Batch 5:  Validation Loss:     1.2885 Validation Accuracy: 0.564600\n",
      "Train Loss:     0.2489 Train Accuracy: 1.000000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Validation Loss:     1.3043 Validation Accuracy: 0.563400\n",
      "Train Loss:     0.3961 Train Accuracy: 0.875000\n",
      "Epoch 23, CIFAR-10 Batch 2:  Validation Loss:     1.2672 Validation Accuracy: 0.569600\n",
      "Train Loss:     0.3430 Train Accuracy: 0.925000\n",
      "Epoch 23, CIFAR-10 Batch 3:  Validation Loss:     1.3107 Validation Accuracy: 0.551800\n",
      "Train Loss:     0.2279 Train Accuracy: 0.975000\n",
      "Epoch 23, CIFAR-10 Batch 4:  Validation Loss:     1.2909 Validation Accuracy: 0.558200\n",
      "Train Loss:     0.3278 Train Accuracy: 0.950000\n",
      "Epoch 23, CIFAR-10 Batch 5:  Validation Loss:     1.3156 Validation Accuracy: 0.558800\n",
      "Train Loss:     0.2622 Train Accuracy: 0.950000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Validation Loss:     1.3254 Validation Accuracy: 0.557800\n",
      "Train Loss:     0.3534 Train Accuracy: 0.950000\n",
      "Epoch 24, CIFAR-10 Batch 2:  Validation Loss:     1.2827 Validation Accuracy: 0.569400\n",
      "Train Loss:     0.3157 Train Accuracy: 0.925000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Validation Loss:     1.3039 Validation Accuracy: 0.557800\n",
      "Train Loss:     0.2493 Train Accuracy: 0.925000\n",
      "Epoch 24, CIFAR-10 Batch 4:  Validation Loss:     1.3073 Validation Accuracy: 0.556000\n",
      "Train Loss:     0.3415 Train Accuracy: 0.925000\n",
      "Epoch 24, CIFAR-10 Batch 5:  Validation Loss:     1.3378 Validation Accuracy: 0.557400\n",
      "Train Loss:     0.2456 Train Accuracy: 0.950000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Validation Loss:     1.3228 Validation Accuracy: 0.561600\n",
      "Train Loss:     0.3233 Train Accuracy: 0.925000\n",
      "Epoch 25, CIFAR-10 Batch 2:  Validation Loss:     1.2773 Validation Accuracy: 0.567200\n",
      "Train Loss:     0.2903 Train Accuracy: 0.925000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, CIFAR-10 Batch 3:  Validation Loss:     1.3123 Validation Accuracy: 0.558400\n",
      "Train Loss:     0.1821 Train Accuracy: 0.975000\n",
      "Epoch 25, CIFAR-10 Batch 4:  Validation Loss:     1.3311 Validation Accuracy: 0.555600\n",
      "Train Loss:     0.2766 Train Accuracy: 0.950000\n",
      "Epoch 25, CIFAR-10 Batch 5:  Validation Loss:     1.3673 Validation Accuracy: 0.562000\n",
      "Train Loss:     0.2085 Train Accuracy: 1.000000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Validation Loss:     1.3509 Validation Accuracy: 0.553000\n",
      "Train Loss:     0.3262 Train Accuracy: 0.950000\n",
      "Epoch 26, CIFAR-10 Batch 2:  Validation Loss:     1.3307 Validation Accuracy: 0.567400\n",
      "Train Loss:     0.3843 Train Accuracy: 0.875000\n",
      "Epoch 26, CIFAR-10 Batch 3:  Validation Loss:     1.3336 Validation Accuracy: 0.557800\n",
      "Train Loss:     0.2010 Train Accuracy: 1.000000\n",
      "Epoch 26, CIFAR-10 Batch 4:  Validation Loss:     1.3758 Validation Accuracy: 0.550800\n",
      "Train Loss:     0.3292 Train Accuracy: 0.950000\n",
      "Epoch 26, CIFAR-10 Batch 5:  Validation Loss:     1.3505 Validation Accuracy: 0.563400\n",
      "Train Loss:     0.2092 Train Accuracy: 0.975000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Validation Loss:     1.3587 Validation Accuracy: 0.555600\n",
      "Train Loss:     0.3270 Train Accuracy: 0.900000\n",
      "Epoch 27, CIFAR-10 Batch 2:  Validation Loss:     1.3315 Validation Accuracy: 0.562000\n",
      "Train Loss:     0.3013 Train Accuracy: 0.925000\n",
      "Epoch 27, CIFAR-10 Batch 3:  Validation Loss:     1.3512 Validation Accuracy: 0.554600\n",
      "Train Loss:     0.1943 Train Accuracy: 0.975000\n",
      "Epoch 27, CIFAR-10 Batch 4:  Validation Loss:     1.3762 Validation Accuracy: 0.546600\n",
      "Train Loss:     0.2981 Train Accuracy: 0.975000\n",
      "Epoch 27, CIFAR-10 Batch 5:  Validation Loss:     1.3662 Validation Accuracy: 0.560000\n",
      "Train Loss:     0.1905 Train Accuracy: 1.000000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Validation Loss:     1.3719 Validation Accuracy: 0.548800\n",
      "Train Loss:     0.2928 Train Accuracy: 0.925000\n",
      "Epoch 28, CIFAR-10 Batch 2:  Validation Loss:     1.3087 Validation Accuracy: 0.574200\n",
      "Train Loss:     0.3292 Train Accuracy: 0.925000\n",
      "Epoch 28, CIFAR-10 Batch 3:  Validation Loss:     1.3666 Validation Accuracy: 0.550800\n",
      "Train Loss:     0.2049 Train Accuracy: 0.975000\n",
      "Epoch 28, CIFAR-10 Batch 4:  Validation Loss:     1.4071 Validation Accuracy: 0.535400\n",
      "Train Loss:     0.3065 Train Accuracy: 0.950000\n",
      "Epoch 28, CIFAR-10 Batch 5:  Validation Loss:     1.4167 Validation Accuracy: 0.552200\n",
      "Train Loss:     0.2027 Train Accuracy: 0.975000\n",
      "Epoch 29, CIFAR-10 Batch 1:  Validation Loss:     1.3954 Validation Accuracy: 0.547600\n",
      "Train Loss:     0.2576 Train Accuracy: 0.975000\n",
      "Epoch 29, CIFAR-10 Batch 2:  Validation Loss:     1.3325 Validation Accuracy: 0.564000\n",
      "Train Loss:     0.2988 Train Accuracy: 0.925000\n",
      "Epoch 29, CIFAR-10 Batch 3:  Validation Loss:     1.3458 Validation Accuracy: 0.555200\n",
      "Train Loss:     0.1706 Train Accuracy: 1.000000\n",
      "Epoch 29, CIFAR-10 Batch 4:  Validation Loss:     1.4182 Validation Accuracy: 0.540400\n",
      "Train Loss:     0.2626 Train Accuracy: 0.950000\n",
      "Epoch 29, CIFAR-10 Batch 5:  Validation Loss:     1.3987 Validation Accuracy: 0.552400\n",
      "Train Loss:     0.1593 Train Accuracy: 1.000000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Validation Loss:     1.3516 Validation Accuracy: 0.556400\n",
      "Train Loss:     0.2667 Train Accuracy: 0.975000\n",
      "Epoch 30, CIFAR-10 Batch 2:  Validation Loss:     1.3433 Validation Accuracy: 0.560400\n",
      "Train Loss:     0.2683 Train Accuracy: 0.925000\n",
      "Epoch 30, CIFAR-10 Batch 3:  Validation Loss:     1.3576 Validation Accuracy: 0.556200\n",
      "Train Loss:     0.2013 Train Accuracy: 0.975000\n",
      "Epoch 30, CIFAR-10 Batch 4:  Validation Loss:     1.4198 Validation Accuracy: 0.542200\n",
      "Train Loss:     0.2930 Train Accuracy: 0.950000\n",
      "Epoch 30, CIFAR-10 Batch 5:  Validation Loss:     1.3733 Validation Accuracy: 0.556800\n",
      "Train Loss:     0.1749 Train Accuracy: 1.000000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Validation Loss:     1.3802 Validation Accuracy: 0.555600\n",
      "Train Loss:     0.2005 Train Accuracy: 1.000000\n",
      "Epoch 31, CIFAR-10 Batch 2:  Validation Loss:     1.3571 Validation Accuracy: 0.553800\n",
      "Train Loss:     0.2931 Train Accuracy: 0.925000\n",
      "Epoch 31, CIFAR-10 Batch 3:  Validation Loss:     1.3802 Validation Accuracy: 0.559200\n",
      "Train Loss:     0.1604 Train Accuracy: 0.975000\n",
      "Epoch 31, CIFAR-10 Batch 4:  Validation Loss:     1.4175 Validation Accuracy: 0.542800\n",
      "Train Loss:     0.2900 Train Accuracy: 0.975000\n",
      "Epoch 31, CIFAR-10 Batch 5:  Validation Loss:     1.3728 Validation Accuracy: 0.562800\n",
      "Train Loss:     0.2024 Train Accuracy: 0.975000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Validation Loss:     1.3524 Validation Accuracy: 0.560600\n",
      "Train Loss:     0.2092 Train Accuracy: 0.975000\n",
      "Epoch 32, CIFAR-10 Batch 2:  Validation Loss:     1.3633 Validation Accuracy: 0.557200\n",
      "Train Loss:     0.2465 Train Accuracy: 0.950000\n",
      "Epoch 32, CIFAR-10 Batch 3:  Validation Loss:     1.3728 Validation Accuracy: 0.559600\n",
      "Train Loss:     0.1351 Train Accuracy: 1.000000\n",
      "Epoch 32, CIFAR-10 Batch 4:  Validation Loss:     1.3240 Validation Accuracy: 0.559600\n",
      "Train Loss:     0.3089 Train Accuracy: 0.925000\n",
      "Epoch 32, CIFAR-10 Batch 5:  Validation Loss:     1.3883 Validation Accuracy: 0.552800\n",
      "Train Loss:     0.1791 Train Accuracy: 1.000000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Validation Loss:     1.3990 Validation Accuracy: 0.553400\n",
      "Train Loss:     0.1646 Train Accuracy: 1.000000\n",
      "Epoch 33, CIFAR-10 Batch 2:  Validation Loss:     1.4410 Validation Accuracy: 0.549200\n",
      "Train Loss:     0.1980 Train Accuracy: 0.975000\n",
      "Epoch 33, CIFAR-10 Batch 3:  Validation Loss:     1.4067 Validation Accuracy: 0.560400\n",
      "Train Loss:     0.1313 Train Accuracy: 0.975000\n",
      "Epoch 33, CIFAR-10 Batch 4:  Validation Loss:     1.3683 Validation Accuracy: 0.564200\n",
      "Train Loss:     0.2007 Train Accuracy: 1.000000\n",
      "Epoch 33, CIFAR-10 Batch 5:  Validation Loss:     1.4164 Validation Accuracy: 0.557200\n",
      "Train Loss:     0.1848 Train Accuracy: 0.975000\n",
      "Epoch 34, CIFAR-10 Batch 1:  Validation Loss:     1.4161 Validation Accuracy: 0.552000\n",
      "Train Loss:     0.1588 Train Accuracy: 0.975000\n",
      "Epoch 34, CIFAR-10 Batch 2:  Validation Loss:     1.4071 Validation Accuracy: 0.554000\n",
      "Train Loss:     0.1676 Train Accuracy: 0.975000\n",
      "Epoch 34, CIFAR-10 Batch 3:  Validation Loss:     1.4200 Validation Accuracy: 0.556200\n",
      "Train Loss:     0.1296 Train Accuracy: 1.000000\n",
      "Epoch 34, CIFAR-10 Batch 4:  Validation Loss:     1.4033 Validation Accuracy: 0.550200\n",
      "Train Loss:     0.2144 Train Accuracy: 0.975000\n",
      "Epoch 34, CIFAR-10 Batch 5:  Validation Loss:     1.4137 Validation Accuracy: 0.550200\n",
      "Train Loss:     0.1715 Train Accuracy: 0.975000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Validation Loss:     1.4336 Validation Accuracy: 0.555400\n",
      "Train Loss:     0.1518 Train Accuracy: 1.000000\n",
      "Epoch 35, CIFAR-10 Batch 2:  Validation Loss:     1.4125 Validation Accuracy: 0.551600\n",
      "Train Loss:     0.1670 Train Accuracy: 0.975000\n",
      "Epoch 35, CIFAR-10 Batch 3:  Validation Loss:     1.4180 Validation Accuracy: 0.553600\n",
      "Train Loss:     0.1154 Train Accuracy: 1.000000\n",
      "Epoch 35, CIFAR-10 Batch 4:  Validation Loss:     1.3637 Validation Accuracy: 0.561000\n",
      "Train Loss:     0.1833 Train Accuracy: 1.000000\n",
      "Epoch 35, CIFAR-10 Batch 5:  Validation Loss:     1.4537 Validation Accuracy: 0.551600\n",
      "Train Loss:     0.1590 Train Accuracy: 1.000000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Validation Loss:     1.4713 Validation Accuracy: 0.552800\n",
      "Train Loss:     0.1247 Train Accuracy: 1.000000\n",
      "Epoch 36, CIFAR-10 Batch 2:  Validation Loss:     1.4541 Validation Accuracy: 0.551000\n",
      "Train Loss:     0.1419 Train Accuracy: 0.950000\n",
      "Epoch 36, CIFAR-10 Batch 3:  Validation Loss:     1.4092 Validation Accuracy: 0.564600\n",
      "Train Loss:     0.1251 Train Accuracy: 1.000000\n",
      "Epoch 36, CIFAR-10 Batch 4:  Validation Loss:     1.4252 Validation Accuracy: 0.561200\n",
      "Train Loss:     0.2033 Train Accuracy: 0.975000\n",
      "Epoch 36, CIFAR-10 Batch 5:  Validation Loss:     1.4668 Validation Accuracy: 0.556000\n",
      "Train Loss:     0.1149 Train Accuracy: 1.000000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Validation Loss:     1.5006 Validation Accuracy: 0.547000\n",
      "Train Loss:     0.1309 Train Accuracy: 1.000000\n",
      "Epoch 37, CIFAR-10 Batch 2:  Validation Loss:     1.4733 Validation Accuracy: 0.541400\n",
      "Train Loss:     0.1272 Train Accuracy: 0.975000\n",
      "Epoch 37, CIFAR-10 Batch 3:  Validation Loss:     1.4463 Validation Accuracy: 0.563000\n",
      "Train Loss:     0.0818 Train Accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, CIFAR-10 Batch 4:  Validation Loss:     1.4563 Validation Accuracy: 0.550600\n",
      "Train Loss:     0.1879 Train Accuracy: 1.000000\n",
      "Epoch 37, CIFAR-10 Batch 5:  Validation Loss:     1.5057 Validation Accuracy: 0.547000\n",
      "Train Loss:     0.0848 Train Accuracy: 1.000000\n",
      "Epoch 38, CIFAR-10 Batch 1:  Validation Loss:     1.4851 Validation Accuracy: 0.554400\n",
      "Train Loss:     0.1285 Train Accuracy: 0.975000\n",
      "Epoch 38, CIFAR-10 Batch 2:  Validation Loss:     1.4816 Validation Accuracy: 0.550000\n",
      "Train Loss:     0.1177 Train Accuracy: 1.000000\n",
      "Epoch 38, CIFAR-10 Batch 3:  Validation Loss:     1.4747 Validation Accuracy: 0.560000\n",
      "Train Loss:     0.1193 Train Accuracy: 0.975000\n",
      "Epoch 38, CIFAR-10 Batch 4:  Validation Loss:     1.4948 Validation Accuracy: 0.551800\n",
      "Train Loss:     0.1795 Train Accuracy: 1.000000\n",
      "Epoch 38, CIFAR-10 Batch 5:  Validation Loss:     1.4881 Validation Accuracy: 0.548400\n",
      "Train Loss:     0.0996 Train Accuracy: 1.000000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Validation Loss:     1.4969 Validation Accuracy: 0.551800\n",
      "Train Loss:     0.1224 Train Accuracy: 1.000000\n",
      "Epoch 39, CIFAR-10 Batch 2:  Validation Loss:     1.5224 Validation Accuracy: 0.542000\n",
      "Train Loss:     0.1003 Train Accuracy: 0.975000\n",
      "Epoch 39, CIFAR-10 Batch 3:  Validation Loss:     1.4821 Validation Accuracy: 0.557200\n",
      "Train Loss:     0.1002 Train Accuracy: 1.000000\n",
      "Epoch 39, CIFAR-10 Batch 4:  Validation Loss:     1.4841 Validation Accuracy: 0.559600\n",
      "Train Loss:     0.1464 Train Accuracy: 1.000000\n",
      "Epoch 39, CIFAR-10 Batch 5:  Validation Loss:     1.5358 Validation Accuracy: 0.547400\n",
      "Train Loss:     0.0975 Train Accuracy: 1.000000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Validation Loss:     1.5047 Validation Accuracy: 0.559400\n",
      "Train Loss:     0.0882 Train Accuracy: 1.000000\n",
      "Epoch 40, CIFAR-10 Batch 2:  Validation Loss:     1.5600 Validation Accuracy: 0.540800\n",
      "Train Loss:     0.1393 Train Accuracy: 0.975000\n",
      "Epoch 40, CIFAR-10 Batch 3:  Validation Loss:     1.4993 Validation Accuracy: 0.562200\n",
      "Train Loss:     0.0784 Train Accuracy: 1.000000\n",
      "Epoch 40, CIFAR-10 Batch 4:  Validation Loss:     1.4925 Validation Accuracy: 0.557600\n",
      "Train Loss:     0.1210 Train Accuracy: 1.000000\n",
      "Epoch 40, CIFAR-10 Batch 5:  Validation Loss:     1.5553 Validation Accuracy: 0.542600\n",
      "Train Loss:     0.1185 Train Accuracy: 1.000000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Validation Loss:     1.5411 Validation Accuracy: 0.547200\n",
      "Train Loss:     0.1160 Train Accuracy: 1.000000\n",
      "Epoch 41, CIFAR-10 Batch 2:  Validation Loss:     1.5735 Validation Accuracy: 0.535800\n",
      "Train Loss:     0.0964 Train Accuracy: 0.975000\n",
      "Epoch 41, CIFAR-10 Batch 3:  Validation Loss:     1.5313 Validation Accuracy: 0.560000\n",
      "Train Loss:     0.0630 Train Accuracy: 1.000000\n",
      "Epoch 41, CIFAR-10 Batch 4:  Validation Loss:     1.5129 Validation Accuracy: 0.555000\n",
      "Train Loss:     0.1090 Train Accuracy: 1.000000\n",
      "Epoch 41, CIFAR-10 Batch 5:  Validation Loss:     1.5620 Validation Accuracy: 0.539600\n",
      "Train Loss:     0.0796 Train Accuracy: 1.000000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Validation Loss:     1.5311 Validation Accuracy: 0.549000\n",
      "Train Loss:     0.0997 Train Accuracy: 0.975000\n",
      "Epoch 42, CIFAR-10 Batch 2:  Validation Loss:     1.5649 Validation Accuracy: 0.550200\n",
      "Train Loss:     0.0788 Train Accuracy: 1.000000\n",
      "Epoch 42, CIFAR-10 Batch 3:  Validation Loss:     1.5495 Validation Accuracy: 0.560600\n",
      "Train Loss:     0.0694 Train Accuracy: 1.000000\n",
      "Epoch 42, CIFAR-10 Batch 4:  Validation Loss:     1.5211 Validation Accuracy: 0.547400\n",
      "Train Loss:     0.1296 Train Accuracy: 1.000000\n",
      "Epoch 42, CIFAR-10 Batch 5:  Validation Loss:     1.5233 Validation Accuracy: 0.541800\n",
      "Train Loss:     0.0905 Train Accuracy: 1.000000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Validation Loss:     1.5513 Validation Accuracy: 0.544800\n",
      "Train Loss:     0.1074 Train Accuracy: 1.000000\n",
      "Epoch 43, CIFAR-10 Batch 2:  Validation Loss:     1.5575 Validation Accuracy: 0.545600\n",
      "Train Loss:     0.0836 Train Accuracy: 1.000000\n",
      "Epoch 43, CIFAR-10 Batch 3:  Validation Loss:     1.5240 Validation Accuracy: 0.559200\n",
      "Train Loss:     0.0936 Train Accuracy: 1.000000\n",
      "Epoch 43, CIFAR-10 Batch 4:  Validation Loss:     1.5942 Validation Accuracy: 0.539600\n",
      "Train Loss:     0.1290 Train Accuracy: 1.000000\n",
      "Epoch 43, CIFAR-10 Batch 5:  Validation Loss:     1.5466 Validation Accuracy: 0.540600\n",
      "Train Loss:     0.0932 Train Accuracy: 1.000000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Validation Loss:     1.5320 Validation Accuracy: 0.549400\n",
      "Train Loss:     0.0998 Train Accuracy: 1.000000\n",
      "Epoch 44, CIFAR-10 Batch 2:  Validation Loss:     1.5524 Validation Accuracy: 0.548800\n",
      "Train Loss:     0.0622 Train Accuracy: 1.000000\n",
      "Epoch 44, CIFAR-10 Batch 3:  Validation Loss:     1.5324 Validation Accuracy: 0.555600\n",
      "Train Loss:     0.0723 Train Accuracy: 1.000000\n",
      "Epoch 44, CIFAR-10 Batch 4:  Validation Loss:     1.5733 Validation Accuracy: 0.543600\n",
      "Train Loss:     0.1247 Train Accuracy: 1.000000\n",
      "Epoch 44, CIFAR-10 Batch 5:  Validation Loss:     1.5385 Validation Accuracy: 0.538800\n",
      "Train Loss:     0.1069 Train Accuracy: 0.975000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Validation Loss:     1.5979 Validation Accuracy: 0.541600\n",
      "Train Loss:     0.0867 Train Accuracy: 1.000000\n",
      "Epoch 45, CIFAR-10 Batch 2:  Validation Loss:     1.5321 Validation Accuracy: 0.544200\n",
      "Train Loss:     0.0515 Train Accuracy: 1.000000\n",
      "Epoch 45, CIFAR-10 Batch 3:  Validation Loss:     1.5748 Validation Accuracy: 0.559800\n",
      "Train Loss:     0.0696 Train Accuracy: 1.000000\n",
      "Epoch 45, CIFAR-10 Batch 4:  Validation Loss:     1.5278 Validation Accuracy: 0.551000\n",
      "Train Loss:     0.1976 Train Accuracy: 0.950000\n",
      "Epoch 45, CIFAR-10 Batch 5:  Validation Loss:     1.5593 Validation Accuracy: 0.544000\n",
      "Train Loss:     0.0877 Train Accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Validation Loss:     1.5931 Validation Accuracy: 0.531000\n",
      "Train Loss:     0.0951 Train Accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 2:  Validation Loss:     1.5462 Validation Accuracy: 0.551600\n",
      "Train Loss:     0.0722 Train Accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 3:  Validation Loss:     1.5902 Validation Accuracy: 0.550000\n",
      "Train Loss:     0.0581 Train Accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 4:  Validation Loss:     1.5923 Validation Accuracy: 0.532800\n",
      "Train Loss:     0.1174 Train Accuracy: 0.975000\n",
      "Epoch 46, CIFAR-10 Batch 5:  Validation Loss:     1.5975 Validation Accuracy: 0.534400\n",
      "Train Loss:     0.0812 Train Accuracy: 1.000000\n",
      "Epoch 47, CIFAR-10 Batch 1:  Validation Loss:     1.6162 Validation Accuracy: 0.543200\n",
      "Train Loss:     0.0922 Train Accuracy: 0.975000\n",
      "Epoch 47, CIFAR-10 Batch 2:  Validation Loss:     1.5399 Validation Accuracy: 0.544800\n",
      "Train Loss:     0.0639 Train Accuracy: 1.000000\n",
      "Epoch 47, CIFAR-10 Batch 3:  Validation Loss:     1.5493 Validation Accuracy: 0.557000\n",
      "Train Loss:     0.0687 Train Accuracy: 1.000000\n",
      "Epoch 47, CIFAR-10 Batch 4:  Validation Loss:     1.5469 Validation Accuracy: 0.547600\n",
      "Train Loss:     0.0880 Train Accuracy: 1.000000\n",
      "Epoch 47, CIFAR-10 Batch 5:  Validation Loss:     1.5881 Validation Accuracy: 0.538000\n",
      "Train Loss:     0.0704 Train Accuracy: 1.000000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Validation Loss:     1.6069 Validation Accuracy: 0.542600\n",
      "Train Loss:     0.0897 Train Accuracy: 1.000000\n",
      "Epoch 48, CIFAR-10 Batch 2:  Validation Loss:     1.5831 Validation Accuracy: 0.542400\n",
      "Train Loss:     0.0468 Train Accuracy: 1.000000\n",
      "Epoch 48, CIFAR-10 Batch 3:  Validation Loss:     1.5855 Validation Accuracy: 0.550400\n",
      "Train Loss:     0.0663 Train Accuracy: 1.000000\n",
      "Epoch 48, CIFAR-10 Batch 4:  Validation Loss:     1.5983 Validation Accuracy: 0.537400\n",
      "Train Loss:     0.1055 Train Accuracy: 0.975000\n",
      "Epoch 48, CIFAR-10 Batch 5:  Validation Loss:     1.6105 Validation Accuracy: 0.542000\n",
      "Train Loss:     0.0775 Train Accuracy: 1.000000\n",
      "Epoch 49, CIFAR-10 Batch 1:  Validation Loss:     1.6194 Validation Accuracy: 0.548800\n",
      "Train Loss:     0.0751 Train Accuracy: 1.000000\n",
      "Epoch 49, CIFAR-10 Batch 2:  Validation Loss:     1.6087 Validation Accuracy: 0.544000\n",
      "Train Loss:     0.0516 Train Accuracy: 1.000000\n",
      "Epoch 49, CIFAR-10 Batch 3:  Validation Loss:     1.5868 Validation Accuracy: 0.549800\n",
      "Train Loss:     0.0586 Train Accuracy: 1.000000\n",
      "Epoch 49, CIFAR-10 Batch 4:  Validation Loss:     1.5450 Validation Accuracy: 0.553600\n",
      "Train Loss:     0.0809 Train Accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, CIFAR-10 Batch 5:  Validation Loss:     1.5864 Validation Accuracy: 0.541200\n",
      "Train Loss:     0.0434 Train Accuracy: 1.000000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Validation Loss:     1.6573 Validation Accuracy: 0.540000\n",
      "Train Loss:     0.0722 Train Accuracy: 1.000000\n",
      "Epoch 50, CIFAR-10 Batch 2:  Validation Loss:     1.6151 Validation Accuracy: 0.548600\n",
      "Train Loss:     0.0484 Train Accuracy: 1.000000\n",
      "Epoch 50, CIFAR-10 Batch 3:  Validation Loss:     1.5806 Validation Accuracy: 0.560000\n",
      "Train Loss:     0.0581 Train Accuracy: 1.000000\n",
      "Epoch 50, CIFAR-10 Batch 4:  Validation Loss:     1.6144 Validation Accuracy: 0.549200\n",
      "Train Loss:     0.0947 Train Accuracy: 1.000000\n",
      "Epoch 50, CIFAR-10 Batch 5:  Validation Loss:     1.6397 Validation Accuracy: 0.539600\n",
      "Train Loss:     0.0521 Train Accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Validation Loss:     1.6820 Validation Accuracy: 0.545000\n",
      "Train Loss:     0.0793 Train Accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 2:  Validation Loss:     1.6498 Validation Accuracy: 0.543000\n",
      "Train Loss:     0.0534 Train Accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 3:  Validation Loss:     1.6401 Validation Accuracy: 0.545600\n",
      "Train Loss:     0.0561 Train Accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 4:  Validation Loss:     1.6280 Validation Accuracy: 0.549200\n",
      "Train Loss:     0.0661 Train Accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 5:  Validation Loss:     1.6779 Validation Accuracy: 0.541400\n",
      "Train Loss:     0.0372 Train Accuracy: 1.000000\n",
      "Epoch 52, CIFAR-10 Batch 1:  Validation Loss:     1.6485 Validation Accuracy: 0.543400\n",
      "Train Loss:     0.0636 Train Accuracy: 1.000000\n",
      "Epoch 52, CIFAR-10 Batch 2:  Validation Loss:     1.6382 Validation Accuracy: 0.541800\n",
      "Train Loss:     0.0490 Train Accuracy: 0.975000\n",
      "Epoch 52, CIFAR-10 Batch 3:  Validation Loss:     1.6590 Validation Accuracy: 0.547400\n",
      "Train Loss:     0.0561 Train Accuracy: 1.000000\n",
      "Epoch 52, CIFAR-10 Batch 4:  Validation Loss:     1.6316 Validation Accuracy: 0.555000\n",
      "Train Loss:     0.0706 Train Accuracy: 1.000000\n",
      "Epoch 52, CIFAR-10 Batch 5:  Validation Loss:     1.7225 Validation Accuracy: 0.537800\n",
      "Train Loss:     0.0478 Train Accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Validation Loss:     1.7203 Validation Accuracy: 0.541800\n",
      "Train Loss:     0.0466 Train Accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 2:  Validation Loss:     1.6718 Validation Accuracy: 0.546400\n",
      "Train Loss:     0.0436 Train Accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 3:  Validation Loss:     1.7168 Validation Accuracy: 0.551000\n",
      "Train Loss:     0.0266 Train Accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 4:  Validation Loss:     1.6563 Validation Accuracy: 0.558800\n",
      "Train Loss:     0.0740 Train Accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 5:  Validation Loss:     1.7126 Validation Accuracy: 0.547800\n",
      "Train Loss:     0.0332 Train Accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Validation Loss:     1.7004 Validation Accuracy: 0.549400\n",
      "Train Loss:     0.0446 Train Accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 2:  Validation Loss:     1.6679 Validation Accuracy: 0.553200\n",
      "Train Loss:     0.0407 Train Accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 3:  Validation Loss:     1.6980 Validation Accuracy: 0.551000\n",
      "Train Loss:     0.0278 Train Accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 4:  Validation Loss:     1.6530 Validation Accuracy: 0.556200\n",
      "Train Loss:     0.0447 Train Accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 5:  Validation Loss:     1.7507 Validation Accuracy: 0.542400\n",
      "Train Loss:     0.0321 Train Accuracy: 1.000000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Validation Loss:     1.7045 Validation Accuracy: 0.553000\n",
      "Train Loss:     0.0743 Train Accuracy: 0.975000\n",
      "Epoch 55, CIFAR-10 Batch 2:  Validation Loss:     1.6364 Validation Accuracy: 0.545000\n",
      "Train Loss:     0.0368 Train Accuracy: 1.000000\n",
      "Epoch 55, CIFAR-10 Batch 3:  Validation Loss:     1.6917 Validation Accuracy: 0.544200\n",
      "Train Loss:     0.0345 Train Accuracy: 1.000000\n",
      "Epoch 55, CIFAR-10 Batch 4:  Validation Loss:     1.6048 Validation Accuracy: 0.561600\n",
      "Train Loss:     0.0502 Train Accuracy: 1.000000\n",
      "Epoch 55, CIFAR-10 Batch 5:  Validation Loss:     1.7314 Validation Accuracy: 0.540800\n",
      "Train Loss:     0.0375 Train Accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Validation Loss:     1.7223 Validation Accuracy: 0.550000\n",
      "Train Loss:     0.0413 Train Accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 2:  Validation Loss:     1.6997 Validation Accuracy: 0.547200\n",
      "Train Loss:     0.0252 Train Accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 3:  Validation Loss:     1.7340 Validation Accuracy: 0.541800\n",
      "Train Loss:     0.0490 Train Accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 4:  Validation Loss:     1.6740 Validation Accuracy: 0.560800\n",
      "Train Loss:     0.0509 Train Accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 5:  Validation Loss:     1.7410 Validation Accuracy: 0.540800\n",
      "Train Loss:     0.0353 Train Accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Validation Loss:     1.7816 Validation Accuracy: 0.540800\n",
      "Train Loss:     0.0479 Train Accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 2:  Validation Loss:     1.7136 Validation Accuracy: 0.543600\n",
      "Train Loss:     0.0252 Train Accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 3:  Validation Loss:     1.7387 Validation Accuracy: 0.542200\n",
      "Train Loss:     0.0321 Train Accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 4:  Validation Loss:     1.7023 Validation Accuracy: 0.551400\n",
      "Train Loss:     0.0330 Train Accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 5:  Validation Loss:     1.7521 Validation Accuracy: 0.546800\n",
      "Train Loss:     0.0346 Train Accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Validation Loss:     1.7080 Validation Accuracy: 0.548400\n",
      "Train Loss:     0.0321 Train Accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Validation Loss:     1.7676 Validation Accuracy: 0.538200\n",
      "Train Loss:     0.0214 Train Accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 3:  Validation Loss:     1.7338 Validation Accuracy: 0.543400\n",
      "Train Loss:     0.0318 Train Accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 4:  Validation Loss:     1.7063 Validation Accuracy: 0.553600\n",
      "Train Loss:     0.0569 Train Accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 5:  Validation Loss:     1.7901 Validation Accuracy: 0.545800\n",
      "Train Loss:     0.0257 Train Accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Validation Loss:     1.7388 Validation Accuracy: 0.547800\n",
      "Train Loss:     0.0330 Train Accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 2:  Validation Loss:     1.6395 Validation Accuracy: 0.553000\n",
      "Train Loss:     0.0358 Train Accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 3:  Validation Loss:     1.7756 Validation Accuracy: 0.540200\n",
      "Train Loss:     0.0210 Train Accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 4:  Validation Loss:     1.6801 Validation Accuracy: 0.556200\n",
      "Train Loss:     0.0325 Train Accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 5:  Validation Loss:     1.7916 Validation Accuracy: 0.541600\n",
      "Train Loss:     0.0288 Train Accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Validation Loss:     1.7087 Validation Accuracy: 0.554800\n",
      "Train Loss:     0.0415 Train Accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 2:  Validation Loss:     1.7070 Validation Accuracy: 0.550400\n",
      "Train Loss:     0.0235 Train Accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 3:  Validation Loss:     1.7534 Validation Accuracy: 0.539800\n",
      "Train Loss:     0.0339 Train Accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 4:  Validation Loss:     1.7363 Validation Accuracy: 0.557400\n",
      "Train Loss:     0.0331 Train Accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 5:  Validation Loss:     1.8169 Validation Accuracy: 0.541200\n",
      "Train Loss:     0.0247 Train Accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Validation Loss:     1.7599 Validation Accuracy: 0.548600\n",
      "Train Loss:     0.0213 Train Accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 2:  Validation Loss:     1.7167 Validation Accuracy: 0.548200\n",
      "Train Loss:     0.0255 Train Accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 3:  Validation Loss:     1.8062 Validation Accuracy: 0.537600\n",
      "Train Loss:     0.0215 Train Accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 4:  Validation Loss:     1.7149 Validation Accuracy: 0.560400\n",
      "Train Loss:     0.0335 Train Accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 5:  Validation Loss:     1.7908 Validation Accuracy: 0.543400\n",
      "Train Loss:     0.0422 Train Accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, CIFAR-10 Batch 1:  Validation Loss:     1.7707 Validation Accuracy: 0.547600\n",
      "Train Loss:     0.0207 Train Accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 2:  Validation Loss:     1.7171 Validation Accuracy: 0.549600\n",
      "Train Loss:     0.0335 Train Accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 3:  Validation Loss:     1.7981 Validation Accuracy: 0.531200\n",
      "Train Loss:     0.0225 Train Accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 4:  Validation Loss:     1.7109 Validation Accuracy: 0.563400\n",
      "Train Loss:     0.0223 Train Accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 5:  Validation Loss:     1.8000 Validation Accuracy: 0.543600\n",
      "Train Loss:     0.0272 Train Accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Validation Loss:     1.7093 Validation Accuracy: 0.548200\n",
      "Train Loss:     0.0328 Train Accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 2:  Validation Loss:     1.7578 Validation Accuracy: 0.536800\n",
      "Train Loss:     0.0259 Train Accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 3:  Validation Loss:     1.8255 Validation Accuracy: 0.531200\n",
      "Train Loss:     0.0334 Train Accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 4:  Validation Loss:     1.7214 Validation Accuracy: 0.552200\n",
      "Train Loss:     0.0285 Train Accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 5:  Validation Loss:     1.8318 Validation Accuracy: 0.549200\n",
      "Train Loss:     0.0350 Train Accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Validation Loss:     1.7733 Validation Accuracy: 0.548600\n",
      "Train Loss:     0.0224 Train Accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 2:  Validation Loss:     1.7194 Validation Accuracy: 0.541600\n",
      "Train Loss:     0.0289 Train Accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 3:  Validation Loss:     1.8018 Validation Accuracy: 0.556200\n",
      "Train Loss:     0.0230 Train Accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 4:  Validation Loss:     1.7441 Validation Accuracy: 0.548600\n",
      "Train Loss:     0.0335 Train Accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 5:  Validation Loss:     1.8071 Validation Accuracy: 0.548000\n",
      "Train Loss:     0.0161 Train Accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Validation Loss:     1.8300 Validation Accuracy: 0.547000\n",
      "Train Loss:     0.0300 Train Accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 2:  Validation Loss:     1.7460 Validation Accuracy: 0.540400\n",
      "Train Loss:     0.0423 Train Accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 3:  Validation Loss:     1.8150 Validation Accuracy: 0.547400\n",
      "Train Loss:     0.0176 Train Accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 4:  Validation Loss:     1.7737 Validation Accuracy: 0.550600\n",
      "Train Loss:     0.0323 Train Accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 5:  Validation Loss:     1.7813 Validation Accuracy: 0.554200\n",
      "Train Loss:     0.0109 Train Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Validation Loss:     1.7908 Validation Accuracy: 0.554800\n",
      "Train Loss:     0.0247 Train Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 2:  Validation Loss:     1.7874 Validation Accuracy: 0.536000\n",
      "Train Loss:     0.0228 Train Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 3:  Validation Loss:     1.8197 Validation Accuracy: 0.542000\n",
      "Train Loss:     0.0282 Train Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 4:  Validation Loss:     1.7636 Validation Accuracy: 0.558800\n",
      "Train Loss:     0.0265 Train Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 5:  Validation Loss:     1.8485 Validation Accuracy: 0.543400\n",
      "Train Loss:     0.0144 Train Accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Validation Loss:     1.8497 Validation Accuracy: 0.550400\n",
      "Train Loss:     0.0187 Train Accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 2:  Validation Loss:     1.8207 Validation Accuracy: 0.536000\n",
      "Train Loss:     0.0179 Train Accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 3:  Validation Loss:     1.8414 Validation Accuracy: 0.539000\n",
      "Train Loss:     0.0168 Train Accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 4:  Validation Loss:     1.8051 Validation Accuracy: 0.547000\n",
      "Train Loss:     0.0485 Train Accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 5:  Validation Loss:     1.8699 Validation Accuracy: 0.543600\n",
      "Train Loss:     0.0119 Train Accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 1:  Validation Loss:     1.8150 Validation Accuracy: 0.549600\n",
      "Train Loss:     0.0673 Train Accuracy: 0.975000\n",
      "Epoch 68, CIFAR-10 Batch 2:  Validation Loss:     1.7717 Validation Accuracy: 0.535400\n",
      "Train Loss:     0.0266 Train Accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 3:  Validation Loss:     1.7831 Validation Accuracy: 0.540400\n",
      "Train Loss:     0.0300 Train Accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 4:  Validation Loss:     1.7854 Validation Accuracy: 0.544600\n",
      "Train Loss:     0.0300 Train Accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 5:  Validation Loss:     1.8801 Validation Accuracy: 0.536200\n",
      "Train Loss:     0.0165 Train Accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Validation Loss:     1.8862 Validation Accuracy: 0.550000\n",
      "Train Loss:     0.0278 Train Accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 2:  Validation Loss:     1.8466 Validation Accuracy: 0.534000\n",
      "Train Loss:     0.0235 Train Accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 3:  Validation Loss:     1.8488 Validation Accuracy: 0.536600\n",
      "Train Loss:     0.0279 Train Accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 4:  Validation Loss:     1.8178 Validation Accuracy: 0.557000\n",
      "Train Loss:     0.0227 Train Accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 5:  Validation Loss:     1.8549 Validation Accuracy: 0.550000\n",
      "Train Loss:     0.0113 Train Accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Validation Loss:     1.8797 Validation Accuracy: 0.552600\n",
      "Train Loss:     0.0283 Train Accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Validation Loss:     1.8352 Validation Accuracy: 0.530200\n",
      "Train Loss:     0.0232 Train Accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 3:  Validation Loss:     1.7894 Validation Accuracy: 0.545400\n",
      "Train Loss:     0.0299 Train Accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 4:  Validation Loss:     1.8111 Validation Accuracy: 0.554600\n",
      "Train Loss:     0.0353 Train Accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 5:  Validation Loss:     1.8679 Validation Accuracy: 0.546400\n",
      "Train Loss:     0.0132 Train Accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Validation Loss:     1.8570 Validation Accuracy: 0.556200\n",
      "Train Loss:     0.0211 Train Accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 2:  Validation Loss:     1.8211 Validation Accuracy: 0.531400\n",
      "Train Loss:     0.0274 Train Accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 3:  Validation Loss:     1.7988 Validation Accuracy: 0.536800\n",
      "Train Loss:     0.0412 Train Accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 4:  Validation Loss:     1.8280 Validation Accuracy: 0.547600\n",
      "Train Loss:     0.0364 Train Accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 5:  Validation Loss:     1.9065 Validation Accuracy: 0.546600\n",
      "Train Loss:     0.0147 Train Accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Validation Loss:     1.8749 Validation Accuracy: 0.553400\n",
      "Train Loss:     0.0251 Train Accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 2:  Validation Loss:     1.8754 Validation Accuracy: 0.528000\n",
      "Train Loss:     0.0163 Train Accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 3:  Validation Loss:     1.8054 Validation Accuracy: 0.543200\n",
      "Train Loss:     0.0208 Train Accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 4:  Validation Loss:     1.8293 Validation Accuracy: 0.548600\n",
      "Train Loss:     0.0275 Train Accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 5:  Validation Loss:     1.8565 Validation Accuracy: 0.546200\n",
      "Train Loss:     0.0226 Train Accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Validation Loss:     1.9588 Validation Accuracy: 0.545800\n",
      "Train Loss:     0.0579 Train Accuracy: 0.975000\n",
      "Epoch 73, CIFAR-10 Batch 2:  Validation Loss:     1.8232 Validation Accuracy: 0.526600\n",
      "Train Loss:     0.0212 Train Accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 3:  Validation Loss:     1.8547 Validation Accuracy: 0.549400\n",
      "Train Loss:     0.0203 Train Accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 4:  Validation Loss:     1.8123 Validation Accuracy: 0.553400\n",
      "Train Loss:     0.0162 Train Accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 5:  Validation Loss:     1.9110 Validation Accuracy: 0.551200\n",
      "Train Loss:     0.0113 Train Accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Validation Loss:     1.9099 Validation Accuracy: 0.557600\n",
      "Train Loss:     0.0225 Train Accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, CIFAR-10 Batch 2:  Validation Loss:     1.8403 Validation Accuracy: 0.537000\n",
      "Train Loss:     0.0076 Train Accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 3:  Validation Loss:     1.8676 Validation Accuracy: 0.549800\n",
      "Train Loss:     0.0072 Train Accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 4:  Validation Loss:     1.8278 Validation Accuracy: 0.551400\n",
      "Train Loss:     0.0213 Train Accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 5:  Validation Loss:     1.9266 Validation Accuracy: 0.548200\n",
      "Train Loss:     0.0348 Train Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Validation Loss:     1.9352 Validation Accuracy: 0.553400\n",
      "Train Loss:     0.0188 Train Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 2:  Validation Loss:     1.8291 Validation Accuracy: 0.541400\n",
      "Train Loss:     0.0153 Train Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 3:  Validation Loss:     1.8501 Validation Accuracy: 0.548200\n",
      "Train Loss:     0.0151 Train Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 4:  Validation Loss:     1.8125 Validation Accuracy: 0.549600\n",
      "Train Loss:     0.0229 Train Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 5:  Validation Loss:     1.9118 Validation Accuracy: 0.547800\n",
      "Train Loss:     0.0112 Train Accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Validation Loss:     1.8875 Validation Accuracy: 0.561200\n",
      "Train Loss:     0.0099 Train Accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 2:  Validation Loss:     1.8971 Validation Accuracy: 0.540400\n",
      "Train Loss:     0.0064 Train Accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 3:  Validation Loss:     1.9255 Validation Accuracy: 0.549000\n",
      "Train Loss:     0.0175 Train Accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 4:  Validation Loss:     1.8066 Validation Accuracy: 0.552000\n",
      "Train Loss:     0.0202 Train Accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 5:  Validation Loss:     1.9320 Validation Accuracy: 0.541200\n",
      "Train Loss:     0.0097 Train Accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Validation Loss:     1.9398 Validation Accuracy: 0.549200\n",
      "Train Loss:     0.0165 Train Accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 2:  Validation Loss:     1.8816 Validation Accuracy: 0.537400\n",
      "Train Loss:     0.0148 Train Accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 3:  Validation Loss:     1.9285 Validation Accuracy: 0.538000\n",
      "Train Loss:     0.0251 Train Accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 4:  Validation Loss:     1.8173 Validation Accuracy: 0.554400\n",
      "Train Loss:     0.0256 Train Accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 5:  Validation Loss:     1.9615 Validation Accuracy: 0.551000\n",
      "Train Loss:     0.0079 Train Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Validation Loss:     1.9691 Validation Accuracy: 0.558800\n",
      "Train Loss:     0.0103 Train Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 2:  Validation Loss:     1.9315 Validation Accuracy: 0.537600\n",
      "Train Loss:     0.0188 Train Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 3:  Validation Loss:     1.9022 Validation Accuracy: 0.544800\n",
      "Train Loss:     0.0202 Train Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 4:  Validation Loss:     1.8543 Validation Accuracy: 0.549400\n",
      "Train Loss:     0.0169 Train Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 5:  Validation Loss:     1.9626 Validation Accuracy: 0.549600\n",
      "Train Loss:     0.0112 Train Accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Validation Loss:     1.9590 Validation Accuracy: 0.549600\n",
      "Train Loss:     0.0092 Train Accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 2:  Validation Loss:     1.8950 Validation Accuracy: 0.540600\n",
      "Train Loss:     0.0191 Train Accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 3:  Validation Loss:     1.9452 Validation Accuracy: 0.543200\n",
      "Train Loss:     0.0185 Train Accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 4:  Validation Loss:     1.8779 Validation Accuracy: 0.549800\n",
      "Train Loss:     0.0257 Train Accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Validation Loss:     1.9302 Validation Accuracy: 0.552400\n",
      "Train Loss:     0.0089 Train Accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Validation Loss:     1.9292 Validation Accuracy: 0.554200\n",
      "Train Loss:     0.0096 Train Accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 2:  Validation Loss:     1.8906 Validation Accuracy: 0.541400\n",
      "Train Loss:     0.0197 Train Accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 3:  Validation Loss:     1.9649 Validation Accuracy: 0.546400\n",
      "Train Loss:     0.0456 Train Accuracy: 0.975000\n",
      "Epoch 80, CIFAR-10 Batch 4:  Validation Loss:     1.8741 Validation Accuracy: 0.546000\n",
      "Train Loss:     0.0212 Train Accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 5:  Validation Loss:     1.9854 Validation Accuracy: 0.541400\n",
      "Train Loss:     0.0130 Train Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Validation Loss:     1.9870 Validation Accuracy: 0.553200\n",
      "Train Loss:     0.0086 Train Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 2:  Validation Loss:     1.9516 Validation Accuracy: 0.539200\n",
      "Train Loss:     0.0203 Train Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 3:  Validation Loss:     1.9689 Validation Accuracy: 0.545600\n",
      "Train Loss:     0.0113 Train Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 4:  Validation Loss:     1.9284 Validation Accuracy: 0.554200\n",
      "Train Loss:     0.0171 Train Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 5:  Validation Loss:     2.0172 Validation Accuracy: 0.542000\n",
      "Train Loss:     0.0133 Train Accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Validation Loss:     2.0587 Validation Accuracy: 0.553000\n",
      "Train Loss:     0.0162 Train Accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 2:  Validation Loss:     1.9936 Validation Accuracy: 0.536800\n",
      "Train Loss:     0.0616 Train Accuracy: 0.975000\n",
      "Epoch 82, CIFAR-10 Batch 3:  Validation Loss:     1.9735 Validation Accuracy: 0.537800\n",
      "Train Loss:     0.0190 Train Accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 4:  Validation Loss:     1.8785 Validation Accuracy: 0.549600\n",
      "Train Loss:     0.0201 Train Accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 5:  Validation Loss:     1.9974 Validation Accuracy: 0.546800\n",
      "Train Loss:     0.0099 Train Accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Validation Loss:     2.0413 Validation Accuracy: 0.555000\n",
      "Train Loss:     0.0068 Train Accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 2:  Validation Loss:     2.0315 Validation Accuracy: 0.535400\n",
      "Train Loss:     0.0213 Train Accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 3:  Validation Loss:     2.0585 Validation Accuracy: 0.530200\n",
      "Train Loss:     0.0281 Train Accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 4:  Validation Loss:     1.9280 Validation Accuracy: 0.551200\n",
      "Train Loss:     0.0271 Train Accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 5:  Validation Loss:     1.9632 Validation Accuracy: 0.546400\n",
      "Train Loss:     0.0116 Train Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Validation Loss:     1.9926 Validation Accuracy: 0.548200\n",
      "Train Loss:     0.0031 Train Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 2:  Validation Loss:     2.0251 Validation Accuracy: 0.534400\n",
      "Train Loss:     0.0313 Train Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 3:  Validation Loss:     2.0671 Validation Accuracy: 0.533000\n",
      "Train Loss:     0.0215 Train Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 4:  Validation Loss:     1.9629 Validation Accuracy: 0.546600\n",
      "Train Loss:     0.0195 Train Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 5:  Validation Loss:     2.0552 Validation Accuracy: 0.538400\n",
      "Train Loss:     0.0101 Train Accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Validation Loss:     2.0559 Validation Accuracy: 0.549600\n",
      "Train Loss:     0.0046 Train Accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 2:  Validation Loss:     2.0110 Validation Accuracy: 0.539000\n",
      "Train Loss:     0.0195 Train Accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 3:  Validation Loss:     2.1530 Validation Accuracy: 0.523200\n",
      "Train Loss:     0.0205 Train Accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 4:  Validation Loss:     1.9736 Validation Accuracy: 0.551200\n",
      "Train Loss:     0.0153 Train Accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 5:  Validation Loss:     2.0020 Validation Accuracy: 0.540000\n",
      "Train Loss:     0.0131 Train Accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Validation Loss:     2.0322 Validation Accuracy: 0.549800\n",
      "Train Loss:     0.0126 Train Accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 2:  Validation Loss:     2.0530 Validation Accuracy: 0.530600\n",
      "Train Loss:     0.0145 Train Accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, CIFAR-10 Batch 3:  Validation Loss:     2.1147 Validation Accuracy: 0.529800\n",
      "Train Loss:     0.0158 Train Accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 4:  Validation Loss:     1.9944 Validation Accuracy: 0.543800\n",
      "Train Loss:     0.0283 Train Accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 5:  Validation Loss:     2.0275 Validation Accuracy: 0.543200\n",
      "Train Loss:     0.0091 Train Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Validation Loss:     2.0517 Validation Accuracy: 0.548600\n",
      "Train Loss:     0.0059 Train Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 2:  Validation Loss:     2.1099 Validation Accuracy: 0.536400\n",
      "Train Loss:     0.0268 Train Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 3:  Validation Loss:     2.0676 Validation Accuracy: 0.536200\n",
      "Train Loss:     0.0184 Train Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 4:  Validation Loss:     1.9789 Validation Accuracy: 0.544200\n",
      "Train Loss:     0.0172 Train Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 5:  Validation Loss:     2.0199 Validation Accuracy: 0.546000\n",
      "Train Loss:     0.0079 Train Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Validation Loss:     2.0960 Validation Accuracy: 0.543200\n",
      "Train Loss:     0.0053 Train Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 2:  Validation Loss:     2.1379 Validation Accuracy: 0.523600\n",
      "Train Loss:     0.0222 Train Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 3:  Validation Loss:     2.1623 Validation Accuracy: 0.528200\n",
      "Train Loss:     0.0112 Train Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 4:  Validation Loss:     2.0001 Validation Accuracy: 0.537800\n",
      "Train Loss:     0.0244 Train Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 5:  Validation Loss:     2.0183 Validation Accuracy: 0.543000\n",
      "Train Loss:     0.0122 Train Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Validation Loss:     2.1431 Validation Accuracy: 0.544200\n",
      "Train Loss:     0.0145 Train Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 2:  Validation Loss:     2.0953 Validation Accuracy: 0.527200\n",
      "Train Loss:     0.0133 Train Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 3:  Validation Loss:     2.1335 Validation Accuracy: 0.530200\n",
      "Train Loss:     0.0106 Train Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 4:  Validation Loss:     2.0396 Validation Accuracy: 0.537800\n",
      "Train Loss:     0.0220 Train Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 5:  Validation Loss:     2.0662 Validation Accuracy: 0.538400\n",
      "Train Loss:     0.0100 Train Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Validation Loss:     2.0982 Validation Accuracy: 0.544600\n",
      "Train Loss:     0.0093 Train Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 2:  Validation Loss:     2.1358 Validation Accuracy: 0.526400\n",
      "Train Loss:     0.0212 Train Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 3:  Validation Loss:     2.1191 Validation Accuracy: 0.540200\n",
      "Train Loss:     0.0190 Train Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 4:  Validation Loss:     2.0271 Validation Accuracy: 0.544800\n",
      "Train Loss:     0.0121 Train Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 5:  Validation Loss:     2.0657 Validation Accuracy: 0.541400\n",
      "Train Loss:     0.0108 Train Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Validation Loss:     2.0903 Validation Accuracy: 0.547600\n",
      "Train Loss:     0.0174 Train Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Validation Loss:     2.1233 Validation Accuracy: 0.536400\n",
      "Train Loss:     0.0174 Train Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 3:  Validation Loss:     2.0985 Validation Accuracy: 0.538800\n",
      "Train Loss:     0.0102 Train Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 4:  Validation Loss:     2.0251 Validation Accuracy: 0.545400\n",
      "Train Loss:     0.0265 Train Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 5:  Validation Loss:     2.0784 Validation Accuracy: 0.542400\n",
      "Train Loss:     0.0071 Train Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Validation Loss:     2.0561 Validation Accuracy: 0.539400\n",
      "Train Loss:     0.0063 Train Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 2:  Validation Loss:     2.0820 Validation Accuracy: 0.532000\n",
      "Train Loss:     0.0232 Train Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 3:  Validation Loss:     2.0683 Validation Accuracy: 0.534000\n",
      "Train Loss:     0.0139 Train Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 4:  Validation Loss:     2.0762 Validation Accuracy: 0.544000\n",
      "Train Loss:     0.0111 Train Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 5:  Validation Loss:     2.0891 Validation Accuracy: 0.542000\n",
      "Train Loss:     0.0081 Train Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Validation Loss:     2.1622 Validation Accuracy: 0.541000\n",
      "Train Loss:     0.0072 Train Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 2:  Validation Loss:     2.0676 Validation Accuracy: 0.538400\n",
      "Train Loss:     0.0119 Train Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 3:  Validation Loss:     2.1462 Validation Accuracy: 0.533400\n",
      "Train Loss:     0.0151 Train Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 4:  Validation Loss:     2.0717 Validation Accuracy: 0.540400\n",
      "Train Loss:     0.0158 Train Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 5:  Validation Loss:     2.1253 Validation Accuracy: 0.542600\n",
      "Train Loss:     0.0053 Train Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Validation Loss:     2.1484 Validation Accuracy: 0.546800\n",
      "Train Loss:     0.0046 Train Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 2:  Validation Loss:     2.1258 Validation Accuracy: 0.536600\n",
      "Train Loss:     0.0070 Train Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 3:  Validation Loss:     2.0362 Validation Accuracy: 0.545800\n",
      "Train Loss:     0.0106 Train Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 4:  Validation Loss:     2.0397 Validation Accuracy: 0.542000\n",
      "Train Loss:     0.0124 Train Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 5:  Validation Loss:     2.0834 Validation Accuracy: 0.540800\n",
      "Train Loss:     0.0079 Train Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Validation Loss:     2.1481 Validation Accuracy: 0.542200\n",
      "Train Loss:     0.0059 Train Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 2:  Validation Loss:     2.1316 Validation Accuracy: 0.535400\n",
      "Train Loss:     0.0176 Train Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 3:  Validation Loss:     2.1276 Validation Accuracy: 0.535200\n",
      "Train Loss:     0.0100 Train Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 4:  Validation Loss:     2.0929 Validation Accuracy: 0.541000\n",
      "Train Loss:     0.0141 Train Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 5:  Validation Loss:     2.0779 Validation Accuracy: 0.543200\n",
      "Train Loss:     0.0173 Train Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Validation Loss:     2.1591 Validation Accuracy: 0.546600\n",
      "Train Loss:     0.0055 Train Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 2:  Validation Loss:     2.1498 Validation Accuracy: 0.534200\n",
      "Train Loss:     0.0148 Train Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 3:  Validation Loss:     2.1324 Validation Accuracy: 0.542200\n",
      "Train Loss:     0.0127 Train Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 4:  Validation Loss:     2.0473 Validation Accuracy: 0.544000\n",
      "Train Loss:     0.0241 Train Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 5:  Validation Loss:     2.0773 Validation Accuracy: 0.544200\n",
      "Train Loss:     0.0065 Train Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Validation Loss:     2.1364 Validation Accuracy: 0.541800\n",
      "Train Loss:     0.0050 Train Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 2:  Validation Loss:     2.1609 Validation Accuracy: 0.536200\n",
      "Train Loss:     0.0081 Train Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 3:  Validation Loss:     2.1748 Validation Accuracy: 0.535800\n",
      "Train Loss:     0.0099 Train Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 4:  Validation Loss:     2.1678 Validation Accuracy: 0.528800\n",
      "Train Loss:     0.0122 Train Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 5:  Validation Loss:     2.0691 Validation Accuracy: 0.544600\n",
      "Train Loss:     0.0021 Train Accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Validation Loss:     2.1442 Validation Accuracy: 0.540200\n",
      "Train Loss:     0.0123 Train Accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 2:  Validation Loss:     2.1047 Validation Accuracy: 0.538400\n",
      "Train Loss:     0.0086 Train Accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 3:  Validation Loss:     2.1066 Validation Accuracy: 0.541400\n",
      "Train Loss:     0.0036 Train Accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, CIFAR-10 Batch 4:  Validation Loss:     2.0974 Validation Accuracy: 0.539400\n",
      "Train Loss:     0.0148 Train Accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 5:  Validation Loss:     2.1513 Validation Accuracy: 0.543000\n",
      "Train Loss:     0.0071 Train Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Validation Loss:     2.1860 Validation Accuracy: 0.542600\n",
      "Train Loss:     0.0052 Train Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 2:  Validation Loss:     2.1830 Validation Accuracy: 0.539600\n",
      "Train Loss:     0.0108 Train Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 3:  Validation Loss:     2.1482 Validation Accuracy: 0.543200\n",
      "Train Loss:     0.0038 Train Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 4:  Validation Loss:     2.1808 Validation Accuracy: 0.541000\n",
      "Train Loss:     0.0092 Train Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 5:  Validation Loss:     2.1699 Validation Accuracy: 0.538600\n",
      "Train Loss:     0.0080 Train Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Validation Loss:     2.1322 Validation Accuracy: 0.547800\n",
      "Train Loss:     0.0049 Train Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 2:  Validation Loss:     2.1960 Validation Accuracy: 0.534600\n",
      "Train Loss:     0.0060 Train Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 3:  Validation Loss:     2.1705 Validation Accuracy: 0.541200\n",
      "Train Loss:     0.0058 Train Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 4:  Validation Loss:     2.1244 Validation Accuracy: 0.535400\n",
      "Train Loss:     0.0198 Train Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 5:  Validation Loss:     2.2002 Validation Accuracy: 0.537800\n",
      "Train Loss:     0.0061 Train Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.5535007911392406\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f673c06fb70>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
